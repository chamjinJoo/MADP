# [SCM/GAT ê¸°ë°˜ Causal Reasoning ëª¨ë¸]

## 1. ì „ì²´ êµ¬ì¡° ê°œìš”
- ë³¸ êµ¬ì¡°ëŠ” Multi-Agent í™˜ê²½(íŠ¹íˆ Dec-POMDP)ì—ì„œ **ì¸ê³¼ ì¶”ë¡ (causal reasoning)**ì„ í†µí•©í•œ Actor-Critic ê³„ì—´ ê°•í™”í•™ìŠµ ëª¨ë¸ì„.
- ì£¼ìš” êµ¬ì„±ìš”ì†Œ:
    - **SCM(Structural Causal Model)**: ì—ì´ì „íŠ¸ ê°„ ì¸ê³¼ê´€ê³„ í–‰ë ¬ í•™ìŠµ
    - **GAT(Graph Attention Network)**: SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ ì–´í…ì…˜ì— ë°˜ì˜í•œ í†µì‹ 
    - **CausalGAT**: SCMê³¼ ì¸ê³¼êµ¬ì¡°ë¥¼ ê³µìœ í•˜ëŠ” GAT
    - **Actor/Critic**: ê° ì—ì´ì „íŠ¸ë³„ ì •ì±…/ê°€ì¹˜ í•¨ìˆ˜
    - **ì¤‘ì•™ì§‘ì¤‘ Critic**: MADDPG ìŠ¤íƒ€ì¼ì˜ ì¤‘ì•™ì§‘ì¤‘ ê°€ì¹˜ í•¨ìˆ˜

---

## 2. ë°ì´í„° íë¦„ ë° ì²˜ë¦¬ ê³¼ì •

### (1) í™˜ê²½ì—ì„œì˜ ë°ì´í„° íë¦„
- ê° stepë§ˆë‹¤ í™˜ê²½(env)ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ê°€ ìƒì„±ë¨:
    - `obs`: ê° ì—ì´ì „íŠ¸ì˜ ê´€ì¸¡ê°’ (obs_dim)
    - `acts`: ê° ì—ì´ì „íŠ¸ì˜ í–‰ë™ (action_dim)
    - `rews`: ê° ì—ì´ì „íŠ¸ì˜ ë³´ìƒ
    - `vals`: ê° ì—ì´ì „íŠ¸ì˜ ê°€ì¹˜ ì¶”ì •ì¹˜
    - `dones`: ì¢…ë£Œ ì—¬ë¶€
- ì´ ë°ì´í„°ë“¤ì€ trajectoryë¡œ ì €ì¥ë˜ì–´, í•™ìŠµ ì‹œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ë¨.

### (2) ëª¨ë¸ ì…ë ¥ ë° ì „ì²˜ë¦¬
- `obs`ëŠ” (batch, agents, obs_dim) í˜•íƒœë¡œ ëª¨ë¸ì— ì…ë ¥ë¨.
- `acts`ëŠ” (batch, agents) ë˜ëŠ” (batch, agents, action_dim) í˜•íƒœë¡œ one-hot encodingë˜ì–´ ì‚¬ìš©ë¨.
- `preprocess_obs` í•¨ìˆ˜ì—ì„œ numpy/tensor íƒ€ì… ë³€í™˜ ë° device ì „ì†¡ì´ ì´ë£¨ì–´ì§.

---

## 3. ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„¸

### (1) SCM (Structural Causal Model)
```python
class SCM(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_dim, num_agents, use_causal_prior=True):
        # ì¸ê³¼êµ¬ì¡° í–‰ë ¬ (í•™ìŠµ/ê³ ì •)
        if use_causal_prior:
            self.causal_matrix = nn.Parameter(torch.eye(num_agents), requires_grad=True)
        else:
            self.causal_matrix = nn.Parameter(torch.randn(num_agents, num_agents), requires_grad=True)
        
        # ê° agentë³„ ì¸ê³¼ ë©”ì»¤ë‹ˆì¦˜
        self.causal_mechanisms = nn.ModuleList([...])
        
        # ê° agentë³„ ë…¸ì´ì¦ˆ ëª¨ë¸
        self.noise_models = nn.ModuleList([...])
```

- ê° ì—ì´ì „íŠ¸ ê°„ ì¸ê³¼ê´€ê³„ í–‰ë ¬(softmax(causal_matrix))ì„ í•™ìŠµí•¨.
- ê° ì—ì´ì „íŠ¸ë³„ë¡œ ê´€ì¸¡+í–‰ë™ì„ ë°›ì•„ ì¸ê³¼ ë©”ì»¤ë‹ˆì¦˜ì„ í†µê³¼ì‹œí‚´.
- ì¸ê³¼êµ¬ì¡° í–‰ë ¬ì„ í†µí•´ ê° ì—ì´ì „íŠ¸ì˜ íš¨ê³¼ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ë§Œë“¦.
- ë…¸ì´ì¦ˆ ëª¨ë¸ì„ í†µí•´ ê´€ì¸¡ê°’ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•¨.

### (2) Graph Attention Layer (GAT) - Enhanced with Causal Bias
```python
class GraphAttentionLayer(nn.Module):
    def __init__(self, input_dim, output_dim, num_heads=4, dropout=0.1, alpha=0.2, concat=True, use_causal_bias=True):
        # ê° headë³„ ì„ í˜•ë³€í™˜
        self.W = nn.Parameter(torch.Tensor(num_heads, input_dim, output_dim))
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.Parameter(torch.Tensor(num_heads, 2 * output_dim, 1))
        # Causal Attention Bias
        if use_causal_bias:
            self.causal_bias = nn.Parameter(torch.Tensor(num_heads, 1, 1))
```

- GAT ë…¼ë¬¸ ê¸°ë°˜ ê·¸ë˜í”„ ì–´í…ì…˜ ë ˆì´ì–´
- **Causal Attention Bias**: SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ ì–´í…ì…˜ ìŠ¤ì½”ì–´ì— ì§ì ‘ ë°˜ì˜
- Multi-head attentionìœ¼ë¡œ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì–´í…ì…˜ ê³„ì‚°
- LeakyReLUë¥¼ ì‚¬ìš©í•œ ì–´í…ì…˜ ìŠ¤ì½”ì–´ ê³„ì‚°

### (3) CausalGAT
```python
class CausalGAT(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=4, num_layers=2, 
                 dropout=0.1, scm_causal_matrix=None, use_causal_bias=True, num_agents=2):
        # GAT layers
        self.gat_layers = nn.ModuleList([...])
        
        # SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ ê³µìœ í•˜ê±°ë‚˜ ë…ë¦½ì ì¸ ì¸ê³¼êµ¬ì¡° ì‚¬ìš©
        if scm_causal_matrix is not None:
            self.causal_structure = scm_causal_matrix  # SCMê³¼ ê³µìœ 
        else:
            self.causal_structure = nn.Parameter(torch.eye(num_agents), requires_grad=True)
```

- SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ ê³µìœ í•˜ì—¬ ì¼ê´€ëœ ì¸ê³¼ê´€ê³„ ëª¨ë¸ë§
- ë‹¨ìˆœí™”ëœ êµ¬ì¡°: ë™ì  ì¸ê³¼êµ¬ì¡° ì¡°ì • ê¸°ëŠ¥ ì œê±°
- ê¸°ë³¸ ì¸ê³¼êµ¬ì¡°ë¥¼ GAT ë ˆì´ì–´ì— ì „ë‹¬í•˜ì—¬ ì²˜ë¦¬

### (4) MultiAgentActorCritic
```python
class MultiAgentActorCritic(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_dim=64, num_agents=2, 
                 use_gat=True, use_causal_gat=True, gat_dim=32, num_heads=4, 
                 dropout=0.1, share_causal_structure=True):
        # SCM (ë¨¼ì € ìƒì„±í•˜ì—¬ ì¸ê³¼êµ¬ì¡° ê³µìœ  ê°€ëŠ¥)
        self.scm = SCM(obs_dim, action_dim, hidden_dim, num_agents)
        
        # GAT (SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ ì–´í…ì…˜ì— ë°˜ì˜)
        if use_gat:
            self.gat = GraphAttentionLayer(obs_dim, gat_dim, num_heads, dropout, concat=False, use_causal_bias=True)
            
        # CausalGAT (SCMì˜ ì¸ê³¼êµ¬ì¡° ê³µìœ )
        if use_causal_gat:
            scm_causal_matrix = self.scm.causal_matrix if share_causal_structure else None
            self.causal_gat = CausalGAT(obs_dim, gat_dim, gat_dim, num_heads, 2, dropout, 
                                       scm_causal_matrix=scm_causal_matrix,
                                       use_causal_bias=True, num_agents=num_agents)
        
        # Actor/Critic networks
        self.actor_networks = nn.ModuleList([...])
        self.critic_networks = nn.ModuleList([...])
        
        # ì¤‘ì•™ì§‘ì¤‘ critic (MADDPG ìŠ¤íƒ€ì¼)
        self.centralized_critic = nn.Sequential([...])
```

- ê° ì—ì´ì „íŠ¸ë³„ë¡œ actor/critic ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì§.
- GATê³¼ CausalGATì˜ ì¶œë ¥ì„ concatenationí•˜ì—¬ actor/critic ì…ë ¥ì— ì‚¬ìš©.
- ì¤‘ì•™ì§‘ì¤‘ critic(MADDPG ìŠ¤íƒ€ì¼)ë„ êµ¬í˜„ë˜ì–´ ìˆìŒ.

---

## 4. Causal Reasoningì˜ ì§„í–‰ ë°©ì‹

### (1) ì¸ê³¼êµ¬ì¡° ê³µìœ  ë©”ì»¤ë‹ˆì¦˜
```python
# SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ CausalGATê³¼ ê³µìœ 
scm_causal_matrix = self.scm.causal_matrix if share_causal_structure else None
self.causal_gat = CausalGAT(..., scm_causal_matrix=scm_causal_matrix)
```

### (2) GAT ì–´í…ì…˜ì— ì¸ê³¼êµ¬ì¡° ë°˜ì˜
```python
# SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ adj_matrixì— ê°€ì¤‘ì¹˜ë¡œ ì ìš©
if self.share_causal_structure:
    causal_structure = self.scm.get_causal_structure()
    causal_adj = causal_structure.unsqueeze(0).expand(batch_size, -1, -1)
    adj_matrix = adj_matrix * causal_adj
```

### (3) Causal Attention Bias ì ìš©
```python
# GraphAttentionLayerì—ì„œ ì¸ê³¼êµ¬ì¡°ë¥¼ ì–´í…ì…˜ ìŠ¤ì½”ì–´ì— ì§ì ‘ ë°˜ì˜
if self.use_causal_bias and causal_structure is not None:
    causal_bias = causal_structure * causal_bias_expanded
    attention_scores = attention_scores + causal_bias
```

### (4) Feature Concatenation
```python
# GATê³¼ CausalGATì˜ featureë¥¼ concatenation
if self.use_causal_gat and self.use_gat:
    causal_features = self.causal_gat(observations, adj_matrix)
    if communication_features is not None:
        communication_features = torch.cat([communication_features, causal_features], dim=-1)
```

### (5) í•™ìŠµ íë¦„
1. **SCM í•™ìŠµ**: í™˜ê²½ ë°ì´í„°ë¡œë¶€í„° agent ê°„ ì¸ê³¼ê´€ê³„ í•™ìŠµ
2. **ì¸ê³¼êµ¬ì¡° ì „íŒŒ**: í•™ìŠµëœ ì¸ê³¼êµ¬ì¡°ê°€ GATì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì— ë°˜ì˜
3. **í†µì‹  ìµœì í™”**: ì¸ê³¼ê´€ê³„ê°€ ê°•í•œ agent ê°„ ë” ë§ì€ ì–´í…ì…˜ ë¶€ì—¬
4. **ì •ì±… ê°œì„ **: ì¸ê³¼êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë” íš¨ê³¼ì ì¸ í†µì‹ ìœ¼ë¡œ ì •ì±… í•™ìŠµ

---

## 5. Loss Function êµ¬ì¡°

### (1) SCM Loss
```python
def compute_scm_loss(self, observations, actions, next_observations):
    scm_predictions = self.scm(observations, actions)
    scm_loss = F.mse_loss(scm_predictions, next_observations)
    return scm_loss
```
- SCMì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ ê´€ì¸¡ê°’ê³¼ ì‹¤ì œ next observation ê°„ì˜ MSE loss
- $\text{SCM Loss} = \text{MSE}(\text{SCM}(obs, acts), next\_obs)$

### (2) Causal Consistency Loss
```python
def compute_causal_consistency_loss(self, causal_structure):
    sparsity_loss = torch.norm(causal_structure, p=1)
    identity = torch.eye(causal_structure.shape[0], device=causal_structure.device)
    identity_loss = F.mse_loss(causal_structure, identity)
    return sparsity_loss + identity_loss
```
- ì¸ê³¼êµ¬ì¡° í–‰ë ¬ì˜ sparsity(L1)ì™€ identity(ìê¸° ìì‹ ì— ëŒ€í•œ ì˜í–¥ë ¥ ìœ ë„) lossì˜ í•©
- $\text{Causal Consistency Loss} = \|C\|_1 + \text{MSE}(C, I)$ 
- ì—¬ê¸°ì„œ $C$ëŠ” softmaxëœ ì¸ê³¼êµ¬ì¡° í–‰ë ¬, $I$ëŠ” ë‹¨ìœ„í–‰ë ¬

### (3) Causal Attention Loss
```python
def compute_causal_attention_loss(self, observations, communication_features):
    if communication_features is None:
        return torch.tensor(0.0, device=observations.device)
    
    # ì¸ê³¼êµ¬ì¡° ê¸°ë°˜ ì˜ˆìƒ í†µì‹  ê°•ë„ ê³„ì‚°
    causal_structure = self.scm.get_causal_structure()
    expected_communication = torch.bmm(
        causal_structure.unsqueeze(0).expand(observations.shape[0], -1, -1),
        observations
    )
    
    # í†µì‹  íŠ¹ì„±ê³¼ ì˜ˆìƒ í†µì‹  ê°„ì˜ ì¼ê´€ì„± loss
    consistency_loss = F.mse_loss(
        communication_features.mean(dim=-1, keepdim=True),
        expected_communication.mean(dim=-1, keepdim=True)
    )
    return consistency_loss
```
- **ëª©ì **: SCMì˜ ì¸ê³¼êµ¬ì¡°ì™€ ì‹¤ì œ í†µì‹  íŠ¹ì„±ì´ ì¼ì¹˜í•˜ë„ë¡ ìœ ë„
- **ì˜ë¯¸**: "ì¸ê³¼ê´€ê³„ê°€ ê°•í•œ agentë“¤ ê°„ì—ëŠ” ì‹¤ì œë¡œë„ ë” ë§ì€ í†µì‹ ì´ ì¼ì–´ë‚˜ì•¼ í•œë‹¤"
- $\text{Causal Attention Loss} = \text{MSE}(\text{comm\_features}, \text{expected\_comm})$

### (4) RL Loss (Actor-Critic)
```python
# Policy loss
policy_loss = -(F.log_softmax(logits, dim=-1).gather(1, act_t.unsqueeze(1)).squeeze(1) * adv_t).mean()

# Value loss
value_loss = F.mse_loss(values, gae_ret_t)

# Entropy
entropy = -(probs * F.log_softmax(logits, dim=-1)).sum(dim=-1).mean()

# Total RL loss
loss_rl = policy_loss + value_coef * value_loss - ent_coef * entropy
```
- ì •ì±… ì†ì‹¤: Advantage ê¸°ë°˜ policy gradient
- ê°€ì¹˜ ì†ì‹¤: MSE(critic, GAE target)
- ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤: ì •ì±…ì˜ íƒí—˜ì„± ìœ ë„
- $\text{RL Loss} = \text{Policy Loss} + \lambda_v \cdot \text{Value Loss} - \lambda_e \cdot \text{Entropy}$

### (5) Integrated Causal Loss
```python
def compute_integrated_causal_loss(self, observations, actions, next_observations, 
                                 communication_features, causal_weight=1.0, attention_weight=0.5):
    # SCM loss
    scm_loss = self.compute_scm_loss(observations, actions, next_observations)
    
    # Causal consistency loss
    causal_structure = self.scm.get_causal_structure()
    consistency_loss = self.compute_causal_consistency_loss(causal_structure)
    
    # Causal attention loss
    attention_loss = self.compute_causal_attention_loss(observations, communication_features)
    
    # í†µí•© loss
    total_loss = causal_weight * scm_loss + consistency_loss + attention_weight * attention_loss
    
    return total_loss
```

### (6) Total Loss
```python
total_loss = integrated_causal_loss + loss_rl
```
- $\text{Total Loss} = \text{Integrated Causal Loss} + \text{RL Loss}$
- $\text{Integrated Causal Loss} = \lambda_c \cdot \text{SCM Loss} + \text{Causal Consistency Loss} + \lambda_a \cdot \text{Causal Attention Loss}$

---

## 6. í•™ìŠµ ë° ì¸ê³¼êµ¬ì¡° ì‹œê°í™”

### (1) í•™ìŠµ ê³¼ì •
```python
# ê° stepë§ˆë‹¤ ì¸ê³¼êµ¬ì¡° ì €ì¥
self.causal_structure_list.append(causal_structure.detach().cpu().numpy())

# í•™ìŠµ í›„ ì‹œê°í™”
self.plot_causal_structure_evolution(output_dir)
```

### (2) ì‹œê°í™” ë‚´ìš©
- **Evolution plot**: ê° entry(ì—ì´ì „íŠ¸ ìŒ)ì˜ softmax weight ë³€í™”
- **Last heatmap**: ë§ˆì§€ë§‰ stepì˜ ì¸ê³¼êµ¬ì¡° í–‰ë ¬
- **Training history**: ê°ì¢… loss ë³€í™” ì¶”ì´ (SCM, Causal Consistency, Causal Attention, RL losses)

---

## 7. ìš”ì•½ ë„ì‹

```mermaid
graph TD;
    subgraph ENV["í™˜ê²½"]
        O1["ê´€ì¸¡(obs)"] -->|ì „ì²˜ë¦¬| M1["ëª¨ë¸"]
        A1["í–‰ë™(acts)"] -->|ì „ì²˜ë¦¬| M1
    end
    
    subgraph MODEL["MultiAgentActorCritic"]
        SCM["SCM"] -->|ì¸ê³¼êµ¬ì¡°| GAT["GAT (Causal Bias)"]
        SCM -->|ì¸ê³¼êµ¬ì¡° ê³µìœ | CGAT["CausalGAT"]
        GAT -->|í†µì‹  feature| CONCAT["Feature Concatenation"]
        CGAT -->|ì¸ê³¼ feature| CONCAT
        CONCAT -->|ê²°í•©ëœ feature| AC1["Actor/Critic"]
    end
    
    AC1 -->|ì •ì±…/ê°€ì¹˜| E1["í™˜ê²½"]
    
    subgraph LOSS["Loss Functions"]
        SCM -->|SCM Loss| L1["Integrated Causal Loss"]
        SCM -->|Causal Consistency Loss| L1
        GAT -->|Causal Attention Loss| L1
        L1 -->|+ RL Loss| L2["Total Loss"]
    end
    
    L2 -->|Backprop| MODEL
```

---

## 8. ì£¼ìš” íŠ¹ì§•

### (1) ì¸ê³¼êµ¬ì¡° ê³µìœ 
- SCMì—ì„œ í•™ìŠµí•œ ì¸ê³¼êµ¬ì¡°ê°€ GATê³¼ CausalGATì—ì„œ ê³µìœ ë¨
- ì¼ê´€ëœ ì¸ê³¼ê´€ê³„ ëª¨ë¸ë§ìœ¼ë¡œ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

### (2) Causal Attention Bias
- SCMì˜ ì¸ê³¼êµ¬ì¡°ë¥¼ GATì˜ ì–´í…ì…˜ ìŠ¤ì½”ì–´ì— ì§ì ‘ ë°˜ì˜
- ì¸ê³¼ê´€ê³„ê°€ ê°•í•œ agent ê°„ ë” ë§ì€ ì–´í…ì…˜ ë¶€ì—¬

### (3) Causal Attention Loss
- SCMì˜ ì¸ê³¼êµ¬ì¡°ì™€ ì‹¤ì œ í†µì‹  íŠ¹ì„± ê°„ì˜ ì¼ê´€ì„± ìœ ë„
- ì˜ë¯¸ìˆëŠ” í†µì‹  íŒ¨í„´ í•™ìŠµ

### (4) Feature Concatenation
- GATê³¼ CausalGATì˜ featureë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë³´ì¡´í•˜ë©´ì„œ ê²°í•©
- ë” í’ë¶€í•œ í†µì‹  ì •ë³´ ì œê³µ

### (5) ë‹¨ìˆœí™”ëœ êµ¬ì¡°
- ë™ì  ì¸ê³¼êµ¬ì¡° ì¡°ì • ê¸°ëŠ¥ ì œê±°ë¡œ ë³µì¡ì„± ê°ì†Œ
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì•„í‚¤í…ì²˜

### (6) ì¤‘ì•™ì§‘ì¤‘ Critic
- MADDPG ìŠ¤íƒ€ì¼ì˜ ì¤‘ì•™ì§‘ì¤‘ ê°€ì¹˜ í•¨ìˆ˜
- ëª¨ë“  agentì˜ ê´€ì¸¡ê³¼ í–‰ë™ì„ ê³ ë ¤í•œ ê°€ì¹˜ ì¶”ì •

### (7) ì¸ê³¼êµ¬ì¡° ì‹œê°í™”
- í•™ìŠµ ê³¼ì •ì—ì„œì˜ ì¸ê³¼êµ¬ì¡° ë³€í™” ì¶”ì 
- í•´ì„ ê°€ëŠ¥í•œ ì¸ê³¼ê´€ê³„ ë¶„ì„

---

## 9. Config íŒŒë¼ë¯¸í„°

### (1) ëª¨ë¸ íŒŒë¼ë¯¸í„°
```yaml
model:
  hidden_dim: 64
  use_causal_prior: true
  use_gat: true
  use_causal_gat: true
  gat_dim: 32
  num_heads: 4
  dropout: 0.1
  share_causal_structure: true
```

### (2) í•™ìŠµ íŒŒë¼ë¯¸í„°
```yaml
params:
  lr: 0.001
  total_steps: 1000
  ep_num: 4
  gamma: 0.99
  gae_lambda: 0.95
  value_coef: 0.5
  ent_coef: 0.01
  causal_weight: 1.0      # SCM loss ê°€ì¤‘ì¹˜
  attention_weight: 0.5   # Causal attention loss ê°€ì¤‘ì¹˜
  cuda: true
```

---

## 10. ì°¸ê³ 
- ë³¸ êµ¬ì¡°ëŠ” Dec-POMDP í™˜ê²½ì—ì„œ ê° ì—ì´ì „íŠ¸ì˜ ê´€ì¸¡/í–‰ë™/ì¸ê³¼ê´€ê³„ë¥¼ í†µí•©ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬, ì¸ê³¼ì  reasoningê³¼ íš¨ìœ¨ì  í˜‘ë™ì„ ë™ì‹œì— ë‹¬ì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨.
- SCMì—ì„œ í•™ìŠµí•œ ì¸ê³¼êµ¬ì¡°ê°€ GATì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì— ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜ë˜ì–´ ë” íš¨ê³¼ì ì¸ multi-agent í•™ìŠµì´ ê°€ëŠ¥í•¨.
- ë‹¨ìˆœí™”ëœ êµ¬ì¡°ë¡œ ì¸í•´ ë””ë²„ê¹…ê³¼ ì´í•´ê°€ ìš©ì´í•˜ë©°, í•µì‹¬ì ì¸ ì¸ê³¼ì¶”ë¡  ê¸°ëŠ¥ì€ ìœ ì§€ë¨.


## ğŸ”§ í™˜ê²½ë³„ ì„¤ì •

### DecTiger
```yaml
dectiger:
  task: dectiger
  obs_dim: 16
  hidden_dim: 64
  gat_dim: 48
  z_dim: 24
  act_dim: 3
  nagents: 2
  use_gat: True
  use_causal_gat: False
  use_rnn: True
```

### MPE Simple Spread
```yaml
mpe:
  task: mpe_simple_spread
  nagents: 3
  hidden_dim: 64
  gat_dim: 64
  z_dim: 32
  use_gat: True
  use_causal_gat: False
  use_rnn: True
```

### Speaker-Listener
```yaml
speaker_listener:
  task: speaker_listener
  max_cycles: 25
  continuous_actions: False
```

## ğŸ¯ ì§€ì› í™˜ê²½

### 1. DecTiger
- **ì„¤ëª…**: ë‘ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ì—¬ í˜¸ë‘ì´ê°€ ìˆëŠ” ë¬¸ì„ ì°¾ëŠ” í™˜ê²½
- **ì—ì´ì „íŠ¸ ìˆ˜**: 2
- **ì•¡ì…˜**: Listen, Open-left, Open-right
- **íŠ¹ì§•**: ë¶€ë¶„ ê´€ì°°, í˜‘ë ¥ í•„ìš”

### 2. MPE (Multi-Agent Particle Environment)
- **Simple Spread**: ì—ì´ì „íŠ¸ë“¤ì´ ëœë“œë§ˆí¬ë¥¼ ë®ëŠ” í˜‘ë ¥ í™˜ê²½
- **Speaker-Listener**: ìŠ¤í”¼ì»¤ê°€ ë¦¬ìŠ¤ë„ˆì—ê²Œ ëª©í‘œ ìœ„ì¹˜ë¥¼ ì „ë‹¬í•˜ëŠ” í†µì‹  í™˜ê²½
- **ì—ì´ì „íŠ¸ ìˆ˜**: 3 (Simple Spread), 2 (Speaker-Listener)
- **íŠ¹ì§•**: ì—°ì† ê³µê°„, ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜

### 3. SMAX (StarCraft Multi-Agent Challenge)
- **ì„¤ëª…**: StarCraft II ê¸°ë°˜ ì „ëµ ê²Œì„ í™˜ê²½
- **ì—ì´ì „íŠ¸ ìˆ˜**: 5-10 (ë§µì— ë”°ë¼)
- **íŠ¹ì§•**: ë³µì¡í•œ ì „ëµ, ë‹¤ì–‘í•œ ìœ ë‹› íƒ€ì…

### 4. Switch
- **ì„¤ëª…**: ì—ì´ì „íŠ¸ë“¤ì´ ìŠ¤ìœ„ì¹˜ë¥¼ ì¡°ì‘í•˜ì—¬ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” í™˜ê²½
- **ì—ì´ì „íŠ¸ ìˆ˜**: 2-4
- **íŠ¹ì§•**: ìˆœì°¨ì  í˜‘ë ¥, ë¶€ë¶„ ê´€ì°°

### 5. Predator-Prey
- **ì„¤ëª…**: í¬ì‹ìë“¤ì´ í˜‘ë ¥í•˜ì—¬ ë¨¹ì´ë¥¼ ì¡ëŠ” í™˜ê²½
- **ì—ì´ì „íŠ¸ ìˆ˜**: 2-4 (í¬ì‹ì)
- **íŠ¹ì§•**: í˜‘ë ¥ ì‚¬ëƒ¥, ì „ëµì  ê³„íš

### 6. Level-Based Foraging
- **ì„¤ëª…**: ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥í•˜ì—¬ ìŒì‹ì„ ìˆ˜ì§‘í•˜ëŠ” í™˜ê²½
- **ì—ì´ì „íŠ¸ ìˆ˜**: 2-8
- **íŠ¹ì§•**: ë ˆë²¨ ê¸°ë°˜ í˜‘ë ¥, ìì› ê²½ìŸ

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ì‹¤í—˜ ê´€ë¦¬

### ìë™ ê²°ê³¼ ì €ì¥

í›ˆë ¨ ì™„ë£Œ í›„ ëª¨ë“  ê²°ê³¼ê°€ ìë™ìœ¼ë¡œ `outputs/YYYY-MM-DD_HH-MM-SS/` í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤:

```
outputs/2025-01-15_14-30-25/
â”œâ”€â”€ config_blicket_2025-01-15_14-30-25.json    # ì‹¤í—˜ ì„¤ì •
â”œâ”€â”€ training_history.png                        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ê·¸ë˜í”„
â”œâ”€â”€ episode_returns.png                         # ì—í”¼ì†Œë“œ ë¦¬í„´ ê·¸ë˜í”„
â”œâ”€â”€ training_history.json                       # í›ˆë ¨ ë°ì´í„° (JSON)
â””â”€â”€ episode_returns.json                        # ì—í”¼ì†Œë“œ ë¦¬í„´ ë°ì´í„° (JSON)
```

### ì„¤ì • íŒŒì¼ êµ¬ì¡°

ì €ì¥ë˜ëŠ” ì„¤ì • íŒŒì¼ì—ëŠ” ë‹¤ìŒ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤:

```json
{
  "env_name": "blicket",
  "env_config": {
    "n_agents": 3,
    "n_blickets": 3,
    "max_steps": 20
  },
  "model_config": {
    "hidden_dim": 64,
    "gat_dim": 32,
    "z_dim": 16,
    "use_gat": true,
    "use_causal_gat": false
  },
  "training_config": {
    "lr": 0.001,
    "gamma": 0.99,
    "total_steps": 2000,
    "mixed_precision": true
  },
  "seed": 42,
  "device": "cuda"
}
```

### ê·¸ë˜í”„ì— í•˜ì´í¼íŒŒë¼ë¯¸í„° í‘œì‹œ

ëª¨ë“  ê·¸ë˜í”„ì˜ ì œëª©ì— ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ìë™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤:

```
Training History - blicket
lr: 0.001 | hidden_dim: 64 | gat_dim: 32 | z_dim: 16 | num_agents: 3 | use_gat: True
```

### ì„¤ì • ë¡œë“œ ë° ì¬í˜„

ì €ì¥ëœ ì„¤ì •ì„ ë¡œë“œí•˜ì—¬ ì‹¤í—˜ì„ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from src.utils import load_experiment_config

# ì„¤ì • ë¡œë“œ
config = load_experiment_config("outputs/2025-01-15_14-30-25/config_blicket_2025-01-15_14-30-25.json")

# ì„¤ì • ì •ë³´ í™•ì¸
print(f"í™˜ê²½: {config['env_name']}")
print(f"í•™ìŠµë¥ : {config['training_config']['lr']}")
print(f"ì‹œë“œ: {config['seed']}")
```

### TensorBoard ë¡œê¹…
```bash
tensorboard --logdir logs
```

### ì£¼ìš” ë©”íŠ¸ë¦­
- VAE Loss (NLL, KL, Cooperation)
- RL Loss (Policy, Value, Entropy)
- Gradient Norms
- Episode Returns
- Success Rates

## ğŸ› ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
1. `batch_size` ì¤„ì´ê¸°
2. `mixed_precision: False`ë¡œ ì„¤ì •
3. `gradient_accumulation_steps` ì¦ê°€

### CUDA ì˜¤ë¥˜
1. PyTorch ë²„ì „ í™•ì¸
2. CUDA ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸
3. `cuda: False`ë¡œ CPU ëª¨ë“œ ì‚¬ìš©

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ìš”ì²­, í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- Chung, J., et al. "A Recurrent Latent Variable Model for Sequential Data." NIPS 2015.
- VeliÄkoviÄ‡, P., et al. "Graph Attention Networks." ICLR 2018.
- Lowe, R., et al. "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments." NIPS 2017.
