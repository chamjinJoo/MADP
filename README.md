# VRNN-GAT Multi-Agent Reinforcement Learning Model (Multi-Head Causal Version)

## ğŸ“‹ **ëª¨ë¸ ê°œìš”**

### **í•µì‹¬ ì•„ì´ë””ì–´**
- **VRNN (Variational RNN)**: ê° ì—ì´ì „íŠ¸ì˜ ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ latent spaceì—ì„œ ëª¨ë¸ë§
- **Multi-Head CausalGATLayer**: 4ê°œì˜ ë…ë¦½ì ì¸ attention headë¡œ ë‹¤ì–‘í•œ ì¶”ë¡  ëŠ¥ë ¥ êµ¬í˜„
- **JSD-based Neighbor Selection**: Jensen-Shannon Divergenceë¥¼ ì´ìš©í•œ ë™ì  neighbor ì„ íƒ
- **Adaptive Loss Balancing**: VAE, RL, Communication lossì˜ ë™ì  ê· í˜• ì¡°ì •

### **ì£¼ìš” íŠ¹ì§•**
- **Dec-POMDP í˜¸í™˜**: ê° ì—ì´ì „íŠ¸ëŠ” ìì‹ ì˜ ê´€ì°°ë§Œ ì ‘ê·¼ ê°€ëŠ¥
- **Multi-Head Attention**: 4ê°œì˜ ë…ë¦½ì ì¸ attention headë¡œ ë‹¤ì–‘í•œ ì¶”ë¡ 
- **End-to-end í•™ìŠµ**: VAE, RL, Communication lossë¥¼ ë™ì‹œì— ìµœì í™”
- **Rolling Error Attention**: ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ì´ë™í‰ê· ì„ GAT attentionì— ë°˜ì˜
- **Layer Normalization**: í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
- **Ablation ì§€ì›**: GAT, CausalGAT, Headë³„ ë¹„í™œì„±í™” ì˜µì…˜

## ğŸ—ï¸ **ëª¨ë¸ ì•„í‚¤í…ì²˜**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi-Head Causal VRNN-GAT Model                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Agent 1       â”‚    â”‚   Agent 2       â”‚    â”‚   Agent N       â”‚
      â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
      â”‚  â”‚   VRNN    â”‚  â”‚    â”‚  â”‚   VRNN    â”‚  â”‚    â”‚  â”‚   VRNN    â”‚  â”‚
      â”‚  â”‚  Cell 1   â”‚  â”‚    â”‚  â”‚  Cell 2   â”‚  â”‚    â”‚  â”‚  Cell N   â”‚  â”‚
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                       â”‚                       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Multi-Head CausalGATLayer    â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head 1: Standard         â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Basic GAT + Delta Bias)   â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 
                    â”‚ â”‚    Head 2: Causal           â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Pairwise Relationships)   â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head 3: Temporal         â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Temporal Dependencies)    â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head 4: Situation-aware  â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Context-dependent)        â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head Fusion              â”‚ â”‚
                    â”‚ â”‚  (4-head Integration)       â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Layer Norm + Dropout     â”‚ â”‚
                    â”‚ â”‚  (Stability Enhancement)    â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                       â”‚                       â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Policy Head 1  â”‚    â”‚  Policy Head 2  â”‚    â”‚  Policy Head N  â”‚
      â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ â”‚   Actor     â”‚ â”‚    â”‚ â”‚   Actor     â”‚ â”‚    â”‚ â”‚   Actor     â”‚ â”‚
      â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚
      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ â”‚   Critic    â”‚ â”‚    â”‚ â”‚   Critic    â”‚ â”‚    â”‚ â”‚   Critic    â”‚ â”‚
      â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚
      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ **ë°ì´í„° í”Œë¡œìš°**

### **1. Observation Processing**
```
Input: obs_t (N, obs_dim)
       a_prev (N, act_dim)  
       h_prev (N, hidden_dim)
       rolling_mean_error (N,) [optional]

Output: h_next, nlls, kls, zs, mus, logvars
```

### **2. VRNN Processing**
```
For each agent i:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ VRNN Cell i:                                            â”‚
  â”‚                                                         â”‚
  â”‚ 1. Prior: h_prev â†’ mu_p, logvar_p                       â”‚
  â”‚ 2. Encoder: [obs_t, h_prev] â†’ mu_q, logvar_q            â”‚
  â”‚ 3. Sampling: z_t ~ N(mu_q, exp(logvar_q))               â”‚
  â”‚ 4. Decoder: [z_t, h_prev] â†’ mu_x, logvar_x              â”‚
  â”‚ 5. RNN: [obs_t, z_t, a_prev] â†’ h_next                   â”‚
  â”‚ 6. Loss: NLL + KL divergence                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **3. Multi-Head CausalGAT Communication**
```
Input: V_nodes = [h_next, zs] (N, hidden_dim + z_dim)
       prev_gat_output (N, head_dim) [optional]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head CausalGATLayer:                              â”‚
â”‚                                                         â”‚
â”‚ Shared Transform:                                       â”‚
â”‚   W1(V) â†’ Wh1 (N, hid_dim)                             â”‚
â”‚   W2(Wh1) â†’ Wh2 (N, out_dim)                           â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 1: Standard Attention                          â”‚ â”‚
â”‚ â”‚   src1 + dst1 + delta_bias â†’ alpha1                 â”‚ â”‚
â”‚ â”‚   H1 = alpha1 @ Wh1 â†’ H1_proj (N, head_dim)        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 2: Causal Attention                            â”‚ â”‚
â”‚ â”‚   For each pair (i,j):                              â”‚ â”‚
â”‚ â”‚     causal_encoder([V[i], V[j]]) â†’ causal_feat     â”‚ â”‚
â”‚ â”‚     causal_attn(causal_feat) â†’ causal_score        â”‚ â”‚
â”‚ â”‚   causal_scores â†’ alpha2 â†’ H2_proj (N, head_dim)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 3: Temporal Attention                          â”‚ â”‚
â”‚ â”‚   temporal_encoder([V, prev_hidden]) â†’ temp_feat   â”‚ â”‚
â”‚ â”‚   temporal_attn(temp_feat) â†’ temp_scores           â”‚ â”‚
â”‚ â”‚   temp_scores â†’ alpha3 â†’ H3_proj (N, head_dim)     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 4: Situation-aware Attention                   â”‚ â”‚
â”‚ â”‚   situation_encoder(V) â†’ situation_feat            â”‚ â”‚
â”‚ â”‚   situation_attn(situation_feat) â†’ situation_scoresâ”‚ â”‚
â”‚ â”‚   situation_scores â†’ alpha4 â†’ H4_proj (N, head_dim)â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ Head Fusion:                                            â”‚
â”‚   [H1_proj, H2_proj, H3_proj, H4_proj] â†’ H_concat     â”‚
â”‚   head_fusion(H_concat) â†’ H_fused                      â”‚
â”‚   layer_norm(H_fused) â†’ H_output                       â”‚
â”‚                                                         â”‚
â”‚ Output: H_output (N, out_dim)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **4. Policy Generation**
```
For each agent i:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Policy Head i:                                          â”‚
  â”‚                                                         â”‚
  â”‚ Input: [obs[i], h_next[i], V_gat[i]]                   â”‚
  â”‚        (obs_dim + hidden_dim + gat_dim)                â”‚
  â”‚                                                         â”‚
  â”‚ Actor: input â†’ action_logits (act_dim)                  â”‚
  â”‚ Critic: input â†’ value (1)                               â”‚
  â”‚                                                         â”‚
  â”‚ Output: policy_logits, value                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ **Loss Functions**

### **1. VAE Loss**
```
L_VAE = nll_coef Ã— NLL + kl_coef Ã— KL + coop_coef Ã— Cooperative_KL

- NLL: Negative Log-Likelihood (reconstruction loss)
- KL: KL divergence between prior and posterior
- Cooperative_KL: Pairwise KL between agent latents
```

### **2. RL Loss**
```
L_RL = Policy_Loss + value_coef Ã— Value_Loss - ent_coef Ã— Entropy

- Policy_Loss: -(log_prob Ã— advantage).mean()
- Value_Loss: MSE(value_pred, returns)
- Entropy: -(probs Ã— log_probs).sum(-1).mean()
```

### **3. Communication Loss**
```
L_Comm = 0 (í˜„ì¬ëŠ” ë¹„í™œì„±í™”)

- Communicationì„ GATë¡œ ëŒ€ì²´í•˜ì—¬ ë‹¨ìˆœí™”
```

### **4. Adaptive Loss Balancing**
```
vae_coef, rl_coef, comm_coef = adaptive_loss_coefficients(loss_vae, loss_rl, loss_comm)
L_Total = vae_coef Ã— L_VAE + rl_coef Ã— L_RL + comm_coef Ã— L_Comm
```

## ğŸ“Š **ëª¨ë¸ í†µê³„**

### **íŒŒë¼ë¯¸í„° ë¶„í¬ (64:48:24 ì„¤ì •, 4 heads)**
```
VRNNCell:                   60,544 (57%)
Multi-Head CausalGATLayer:  18,432 (17%)
Policy Heads:               32,040 (30%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                     111,016 (100%)
```

### **ì°¨ì› ì •ë³´**
```
Input:     obs_dim = 16
Hidden:    hidden_dim = 64
Latent:    z_dim = 24
GAT:       gat_dim = 48
Head:      head_dim = 12 (gat_dim // 4)
Action:    act_dim = 3
Agents:    n_agents = 2
Heads:     n_heads = 4
```

## ğŸ”§ **ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**

### **ëª¨ë¸ êµ¬ì¡°**
- `hidden_dim : gat_dim : z_dim = 2.7 : 2 : 1`
- `n_heads = 4`: 4ê°œì˜ ë…ë¦½ì ì¸ attention head
- `head_dim = gat_dim // n_heads = 12`: ê° headì˜ ì°¨ì›
- Rolling window size: 10
- JSD update frequency: ë§¤ ìŠ¤í…

### **Multi-Head CausalGATLayer ì„¤ì •**
- **Head 1**: Standard attention (ê¸°ë³¸ GAT + delta bias)
- **Head 2**: Causal attention (pairwise relationships)
- **Head 3**: Temporal attention (temporal dependencies)
- **Head 4**: Situation-aware attention (context-dependent)
- **Head Fusion**: 4ê°œ head ì¶œë ¥ ê²°í•©
- **Layer Normalization**: í•™ìŠµ ì•ˆì •ì„±
- **Dropout**: 0.6

### **í•™ìŠµ ì„¤ì •**
- Learning rate: 3e-4 (RL), 1e-4 (VAE)
- Loss coefficients: nll_coef=1.0, kl_coef=0.1, coop_coef=0.01
- GAE parameters: Î³=0.99, Î»=0.95
- Gradient clipping: max_grad_norm=0.5
- EMA: ema_alpha=0.99

## ğŸš€ **ì‚¬ìš©ë²•**

### **ëª¨ë¸ ìƒì„±**
```python
model = VRNNGATA2C(
    obs_dim=16,
    act_dim=3,
    hidden_dim=64,
    z_dim=24,
    gat_dim=48,
    n_agents=2,
    use_gat=True,
    use_causal_gat=True,  # Multi-head CausalGATLayer ì‚¬ìš©
)
```

### **Forward Pass**
```python
h_next, nlls, kls, logits, ref_logits, values, mus, logvars, zs, V_gat, comm_recons = \
    model.forward_step(obs, a_prev, h_prev, rolling_mean_error)
```

### **Config ì„¤ì •**
```yaml
dectiger:
  use_causal_gat: true
  n_heads: 4
  max_grad_norm: 0.5
  ema_alpha: 0.99
```

## ğŸ¯ **ì¥ì **

1. **ë‹¤ì–‘í•œ ì¶”ë¡  ëŠ¥ë ¥**: 4ê°œì˜ ë…ë¦½ì ì¸ attention headë¡œ ì„œë¡œ ë‹¤ë¥¸ ì¶”ë¡  ìˆ˜í–‰
2. **ì¸ê³¼ê´€ê³„ ëª¨ë¸ë§**: Causal attentionìœ¼ë¡œ pairwise relationships í•™ìŠµ
3. **ì‹œê°„ì  ì˜ì¡´ì„±**: Temporal attentionìœ¼ë¡œ ì‹œí€€ìŠ¤ ì •ë³´ í™œìš©
4. **ìƒí™© ì¸ì‹**: Situation-aware attentionìœ¼ë¡œ context ê³ ë ¤
5. **í•™ìŠµ ì•ˆì •ì„±**: Layer normalization, dropout, head fusion
6. **í‘œí˜„ë ¥**: VRNNìœ¼ë¡œ ì‹œí€€ìŠ¤ ì •ë³´ ëª¨ë¸ë§
7. **í†µì‹ **: GATë¡œ íš¨ìœ¨ì ì¸ agent ê°„ ì •ë³´ êµí™˜
8. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ablation study ì§€ì›
9. **í™•ì¥ì„±**: ë‹¤ì–‘í•œ í™˜ê²½ì— ì ìš© ê°€ëŠ¥

## ğŸ”¬ **Ablation Study ì˜µì…˜**

1. **Headë³„ Ablation**: ê° attention head ê°œë³„ ë¹„í™œì„±í™”
2. **Standard GAT vs Multi-Head CausalGAT**: `use_causal_gat`
3. **GAT Ablation**: `use_gat=False`
4. **Communication Ablation**: Communication loss ì œê±°
5. **Temporal Reasoning Ablation**: prev_hidden=None
6. **Head Fusion Ablation**: ë‹¨ìˆœ concatenation vs fusion layer
7. **Layer Norm Ablation**: normalization ì œê±°

## ğŸ§  **ê° Headì˜ ì—­í• **

### **Head 1: Standard Attention**
- ê¸°ë³¸ì ì¸ GAT attention mechanism
- Rolling error biasë¡œ ì˜ˆì¸¡ ì˜¤ì°¨ ë°˜ì˜
- ì—ì´ì „íŠ¸ ê°„ ê¸°ë³¸ì ì¸ ì •ë³´ êµí™˜

### **Head 2: Causal Attention**
- Pairwise causal relationships ëª¨ë¸ë§
- ì—ì´ì „íŠ¸ ê°„ ì¸ê³¼ê´€ê³„ í•™ìŠµ
- "Aê°€ Bì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥" ê°™ì€ ì¸ê³¼ì  ê´€ê³„ íŒŒì•…

### **Head 3: Temporal Attention**
- ì‹œê°„ì  ì˜ì¡´ì„± ëª¨ë¸ë§
- ì´ì „ GAT ì¶œë ¥ì„ í™œìš©í•œ temporal reasoning
- ì‹œí€€ìŠ¤ ì •ë³´ì˜ ì‹œê°„ì  íŒ¨í„´ í•™ìŠµ

### **Head 4: Situation-aware Attention**
- í˜„ì¬ ìƒí™©ì— ë”°ë¥¸ context-dependent attention
- ê° ì—ì´ì „íŠ¸ì˜ ìƒí™©ì„ ê³ ë ¤í•œ attention
- í™˜ê²½ ë³€í™”ì— ë”°ë¥¸ ì ì‘ì  ì •ë³´ êµí™˜
