# configs_att.yaml - Attention 기반 멀티에이전트 학습 설정 (UTF-8 인코딩)
seed: 42

# GPU 및 성능 최적화 설정
params:
  cuda: False  # GPU 사용 활성화
  device: "cuda"  # 명시적 디바이스 지정
  mixed_precision: True  # Mixed precision 사용
  max_grad_norm: 1.0  # 그래디언트 클리핑

# 로깅 설정
logger:
  logger: tensorboard 
  logdir: logs
  log_interval: 10  # 로깅 간격
  save_interval: 100  # 모델 저장 간격

# 모델 하이퍼파라미터 (Attention 기반)
model:
  use_attention: True  # Attention 통신 모듈 사용
  use_rnn: True  # VRNN 사용
  hidden_dim: 64  # VRNN hidden dimension
  z_dim: 16  # Latent dimension
  d_model: 32  # Attention model dimension (기존 gat_dim 대체)
  act_dim: 3  # Action dimension

# 학습 하이퍼파라미터
training:
  lr: 3e-4  # Actor-Critic 학습률
  lr_vae: 1e-4  # VAE 학습률
  gamma: 0.99  # 할인 팩터
  gae_lambda: 0.95  # GAE 람다
  ent_coef: 0.01  # 엔트로피 계수
  value_coef: 1.0  # 가치 함수 계수
  nll_coef: 1.0  # NLL 계수 (재구성 손실)
  kl_coef: 0.1  # KL 계수 (VAE 정규화)
  coop_coef: 0.0  # 협력 계수 (에이전트 간 KL)
  infer_coef: 0.1  # Attention inference loss 계수
  total_steps: 1000  # 총 학습 스텝
  ep_num: 2  # 배치당 에피소드 수

# 환경별 설정
dectiger:
  task: dectiger
  run: {steps: 2, train_ratio: 64}
  maxtimestep: 2
  obs_dim: 16
  nagents: 2

rware:
  task: rware
  nagents: 2
  layout: |
    .x..x.
    .x..x.
    ......
    ..gg..

mpe:
  task: mpe_simple_spread
  nagents: 3
  local_ratio: 0.5
  max_cycles: 25
  continuous_actions: False

smax:
  task: smax # 2s3z
  nagents: 5

switch:
  task: switch
  nagents: 2

pp:
  task: pp
  nagents: 2

foraging:
  task: foraging
  nagents: 3

blicket:
  task: blicket
  nagents: 3
  n_blickets: 3
  reward_structure: "quiz-type"
  quiz_positive_reward: 1.0
  quiz_negative_reward: -1.0
  quiz_disabled_steps: 5 