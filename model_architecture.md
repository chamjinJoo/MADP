# [SCM/GAT ê¸°ë°˜ Causal Reasoning ëª¨ë¸]
## 1. ì „ì²´ êµ¬ì¡° ê°œìš”
- ë³¸ êµ¬ì¡°ëŠ” Multi-Agent í™˜ê²½(íŠ¹íˆ Dec-POMDP)ì—ì„œ **ì¸ê³¼ ì¶”ë¡ (causal reasoning)**ì„ í†µí•©í•œ Actor-Critic ê³„ì—´ ê°•í™”í•™ìŠµ ëª¨ë¸ì„.
- ì£¼ìš” êµ¬ì„±ìš”ì†Œ:
    - **SCM(Structural Causal Model)**: ì—ì´ì „íŠ¸ ê°„ ì¸ê³¼ê´€ê³„ í–‰ë ¬ í•™ìŠµ
    - **GAT(Graph Attention Network)**: ì—ì´ì „íŠ¸ ê°„ ì •ë³´ êµí™˜(ì»¤ë®¤ë‹ˆì¼€ì´ì…˜)
    - **CausalGAT**: ì¸ê³¼êµ¬ì¡°ë¥¼ ë°˜ì˜í•œ GAT
    - **Actor/Critic**: ê° ì—ì´ì „íŠ¸ë³„ ì •ì±…/ê°€ì¹˜ í•¨ìˆ˜
    - 
---

## 2. ë°ì´í„° íë¦„ ë° ì²˜ë¦¬ ê³¼ì •

### (1) í™˜ê²½ì—ì„œì˜ ë°ì´í„° íë¦„
- ê° stepë§ˆë‹¤ í™˜ê²½(env)ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ê°€ ìƒì„±ë¨:
    - `obs`: ê° ì—ì´ì „íŠ¸ì˜ ê´€ì¸¡ê°’ (obs_dim)
    - `acts`: ê° ì—ì´ì „íŠ¸ì˜ í–‰ë™ (action_dim)
    - `rews`: ê° ì—ì´ì „íŠ¸ì˜ ë³´ìƒ
    - `vals`: ê° ì—ì´ì „íŠ¸ì˜ ê°€ì¹˜ ì¶”ì •ì¹˜
    - `dones`: ì¢…ë£Œ ì—¬ë¶€
- ì´ ë°ì´í„°ë“¤ì€ trajectoryë¡œ ì €ì¥ë˜ì–´, í•™ìŠµ ì‹œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ë¨.

### (2) ëª¨ë¸ ì…ë ¥ ë° ì „ì²˜ë¦¬
- `obs`ëŠ” (batch, agents, obs_dim) í˜•íƒœë¡œ ëª¨ë¸ì— ì…ë ¥ë¨.
- `acts`ëŠ” (batch, agents) ë˜ëŠ” (batch, agents, action_dim) í˜•íƒœë¡œ one-hot encodingë˜ì–´ ì‚¬ìš©ë¨.
- `preprocess_obs` í•¨ìˆ˜ì—ì„œ numpy/tensor íƒ€ì… ë³€í™˜ ë° device ì „ì†¡ì´ ì´ë£¨ì–´ì§.

---

## 3. ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„¸

### (1) SCM (Structural Causal Model)
- ê° ì—ì´ì „íŠ¸ ê°„ ì¸ê³¼ê´€ê³„ í–‰ë ¬(softmax(causal_matrix))ì„ í•™ìŠµí•¨.
- ê° ì—ì´ì „íŠ¸ë³„ë¡œ ê´€ì¸¡+í–‰ë™ì„ ë°›ì•„ ì¸ê³¼ ë©”ì»¤ë‹ˆì¦˜ì„ í†µê³¼ì‹œí‚´.
- ì¸ê³¼êµ¬ì¡° í–‰ë ¬ì„ í†µí•´ ê° ì—ì´ì „íŠ¸ì˜ íš¨ê³¼ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ë§Œë“¦.
- ë…¸ì´ì¦ˆ ëª¨ë¸ì„ í†µí•´ ê´€ì¸¡ê°’ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•¨.

### (2) GAT / CausalGAT
- GAT: ì—ì´ì „íŠ¸ ê°„ì˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ attention ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ë§.
- CausalGAT: ì¸ê³¼êµ¬ì¡° í–‰ë ¬ì„ softmaxë¡œ ë³€í™˜í•˜ì—¬ ì…ë ¥ì— ê³±í•œ ë’¤ GATì— ì „ë‹¬, ì¸ê³¼ì  ì •ë³´ íë¦„ì„ ë°˜ì˜.

### (3) MultiAgentActorCritic
- ê° ì—ì´ì „íŠ¸ë³„ë¡œ actor/critic ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ì§.
- GAT/CausalGATì˜ ì¶œë ¥(feature)ì„ actor/critic ì…ë ¥ì— concatí•˜ì—¬ ì‚¬ìš©.
- ì¤‘ì•™ì§‘ì¤‘ critic(MADDPG ìŠ¤íƒ€ì¼)ë„ êµ¬í˜„ë˜ì–´ ìˆìŒ.
- forward ì‹œ SCM, GAT, CausalGAT, actor, critic, centralized critic, communication feature, causal structure ë“±ì„ ëª¨ë‘ ë°˜í™˜.

---

## 4. Causal Reasoningì˜ ì§„í–‰ ë°©ì‹
- SCMì˜ ì¸ê³¼êµ¬ì¡° í–‰ë ¬(softmax(causal_matrix))ì´ í•™ìŠµì„ í†µí•´ ê° ì—ì´ì „íŠ¸ ê°„ ì¸ê³¼ì  ì˜í–¥ë ¥ì„ í‘œí˜„í•¨.
- CausalGATì—ì„œëŠ” ì´ ì¸ê³¼êµ¬ì¡°ë¥¼ ì…ë ¥ featureì— ê³±í•´ attentionì— ë°˜ì˜í•¨ìœ¼ë¡œì¨, ì¸ê³¼ì  ì •ë³´ íë¦„ì´ ë„¤íŠ¸ì›Œí¬ ì „ì²´ì— ë°˜ì˜ë¨.
- í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ ì¸ê³¼êµ¬ì¡° í–‰ë ¬ì´ ë³€í™”í•˜ë©°, ì´ëŠ” ì‹œê°í™”(heatmap, evolution plot)ë¡œ í™•ì¸ ê°€ëŠ¥.

---

## 5. Loss Function êµ¬ì¡°

### (1) SCM Loss
- SCMì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ ê´€ì¸¡ê°’ê³¼ ì‹¤ì œ next observation ê°„ì˜ MSE loss
- $\text{SCM Loss} = \text{MSE}(\text{SCM}(obs, acts), next\_obs)$

### (2) Causal Consistency Loss
- ì¸ê³¼êµ¬ì¡° í–‰ë ¬ì˜ sparsity(L1)ì™€ identity(ìê¸° ìì‹ ì— ëŒ€í•œ ì˜í–¥ë ¥ ìœ ë„) lossì˜ í•©
- $\text{Causal Consistency Loss} = \|C\|_1 + \text{MSE}(C, I)$ 
- ì—¬ê¸°ì„œ $C$ëŠ” softmaxëœ ì¸ê³¼êµ¬ì¡° í–‰ë ¬, $I$ëŠ” ë‹¨ìœ„í–‰ë ¬

### (3) RL Loss (Actor-Critic)
- ì •ì±… ì†ì‹¤: Advantage ê¸°ë°˜ policy gradient
- ê°€ì¹˜ ì†ì‹¤: MSE(critic, GAE target)
- ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤: ì •ì±…ì˜ íƒí—˜ì„± ìœ ë„
- $\text{RL Loss} = \text{Policy Loss} + \lambda_v \cdot \text{Value Loss} - \lambda_e \cdot \text{Entropy}$

### (4) Total Loss
- $\text{Total Loss} = \text{SCM Loss} + \text{Causal Consistency Loss} + \text{RL Loss}$

---

## 6. í•™ìŠµ ë° ì¸ê³¼êµ¬ì¡° ì‹œê°í™”
- í•™ìŠµ ì¤‘ ê° stepë§ˆë‹¤ ì¸ê³¼êµ¬ì¡° í–‰ë ¬ì„ ì €ì¥í•˜ì—¬, í•™ìŠµì´ ëë‚œ í›„ ë³€í™” ì¶”ì´ì™€ ìµœì¢… êµ¬ì¡°ë¥¼ ì‹œê°í™”í•¨.
- evolution plot: ê° entry(ì—ì´ì „íŠ¸ ìŒ)ì˜ softmax weight ë³€í™”
- last heatmap: ë§ˆì§€ë§‰ stepì˜ ì¸ê³¼êµ¬ì¡° í–‰ë ¬

---

## 7. ìš”ì•½ ë„ì‹
```mermaid
graph TD;
    subgraph ENV["í™˜ê²½"]
        O1["ê´€ì¸¡(obs)"] -->|ì „ì²˜ë¦¬| M1["ëª¨ë¸"]
        A1["í–‰ë™(acts)"] -->|ì „ì²˜ë¦¬| M1
    end
    M1 -->|SCM, GAT, CausalGAT| AC1["Actor/Critic"]
    AC1 -->|ì •ì±…/ê°€ì¹˜| E1["í™˜ê²½"]
    M1 -->|SCM Loss, Causal Consistency Loss| L1["Total Loss"]
    AC1 -->|RL Loss| L1
```

---

## 8. ì°¸ê³ 
- ë³¸ êµ¬ì¡°ëŠ” Dec-POMDP í™˜ê²½ì—ì„œ ê° ì—ì´ì „íŠ¸ì˜ ê´€ì¸¡/í–‰ë™/ì¸ê³¼ê´€ê³„ë¥¼ í†µí•©ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬, ì¸ê³¼ì  reasoningê³¼ íš¨ìœ¨ì  í˜‘ë™ì„ ë™ì‹œì— ë‹¬ì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨.

---

# 1. [ê¸°ë³¸ VRNN-GAT ëª¨ë¸]
## ğŸ“‹ **ëª¨ë¸ ê°œìš”**

### **í•µì‹¬ ì•„ì´ë””ì–´**
- **VRNN (Variational RNN)**: ê° ì—ì´ì „íŠ¸ì˜ ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ latent spaceì—ì„œ ëª¨ë¸ë§
- **Multi-Head CausalGATLayer**: 4ê°œì˜ ë…ë¦½ì ì¸ attention headë¡œ ë‹¤ì–‘í•œ ì¶”ë¡  ëŠ¥ë ¥ êµ¬í˜„
- **JSD-based Neighbor Selection**: Jensen-Shannon Divergenceë¥¼ ì´ìš©í•œ ë™ì  neighbor ì„ íƒ
- **Adaptive Loss Balancing**: VAE, RL, Communication lossì˜ ë™ì  ê· í˜• ì¡°ì •

### **ì£¼ìš” íŠ¹ì§•**
- **Dec-POMDP í˜¸í™˜**: ê° ì—ì´ì „íŠ¸ëŠ” ìì‹ ì˜ ê´€ì°°ë§Œ ì ‘ê·¼ ê°€ëŠ¥
- **Multi-Head Attention**: 4ê°œì˜ ë…ë¦½ì ì¸ attention headë¡œ ë‹¤ì–‘í•œ ì¶”ë¡ 
- **End-to-end í•™ìŠµ**: VAE, RL, Communication lossë¥¼ ë™ì‹œì— ìµœì í™”
- **Rolling Error Attention**: ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ì´ë™í‰ê· ì„ GAT attentionì— ë°˜ì˜
- **Layer Normalization**: í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
- **Ablation ì§€ì›**: GAT, CausalGAT, Headë³„ ë¹„í™œì„±í™” ì˜µì…˜

## ğŸ—ï¸ **ëª¨ë¸ ì•„í‚¤í…ì²˜**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi-Head Causal VRNN-GAT Model                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Agent 1       â”‚    â”‚   Agent 2       â”‚    â”‚   Agent N       â”‚
      â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
      â”‚  â”‚   VRNN    â”‚  â”‚    â”‚  â”‚   VRNN    â”‚  â”‚    â”‚  â”‚   VRNN    â”‚  â”‚
      â”‚  â”‚  Cell 1   â”‚  â”‚    â”‚  â”‚  Cell 2   â”‚  â”‚    â”‚  â”‚  Cell N   â”‚  â”‚
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                       â”‚                       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Multi-Head CausalGATLayer    â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head 1: Standard         â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Basic GAT + Delta Bias)   â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ 
                    â”‚ â”‚    Head 2: Causal           â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Pairwise Relationships)   â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head 3: Temporal         â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Temporal Dependencies)    â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head 4: Situation-aware  â”‚ â”‚
                    â”‚ â”‚      Attention              â”‚ â”‚
                    â”‚ â”‚  (Context-dependent)        â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Head Fusion              â”‚ â”‚
                    â”‚ â”‚  (4-head Integration)       â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚                                 â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚    Layer Norm + Dropout     â”‚ â”‚
                    â”‚ â”‚  (Stability Enhancement)    â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                       â”‚                       â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Policy Head 1  â”‚    â”‚  Policy Head 2  â”‚    â”‚  Policy Head N  â”‚
      â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ â”‚   Actor     â”‚ â”‚    â”‚ â”‚   Actor     â”‚ â”‚    â”‚ â”‚   Actor     â”‚ â”‚
      â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚
      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ â”‚   Critic    â”‚ â”‚    â”‚ â”‚   Critic    â”‚ â”‚    â”‚ â”‚   Critic    â”‚ â”‚
      â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚    â”‚ â”‚  Network    â”‚ â”‚
      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ **ë°ì´í„° í”Œë¡œìš°**

### **1. Observation Processing**
```
Input: obs_t (N, obs_dim)
       a_prev (N, act_dim)  
       h_prev (N, hidden_dim)
       rolling_mean_error (N,) [optional]

Output: h_next, nlls, kls, zs, mus, logvars
```

### **2. VRNN Processing**
```
For each agent i:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ VRNN Cell i:                                            â”‚
  â”‚                                                         â”‚
  â”‚ 1. Prior: h_prev â†’ mu_p, logvar_p                       â”‚
  â”‚ 2. Encoder: [obs_t, h_prev] â†’ mu_q, logvar_q            â”‚
  â”‚ 3. Sampling: z_t ~ N(mu_q, exp(logvar_q))               â”‚
  â”‚ 4. Decoder: [z_t, h_prev] â†’ mu_x, logvar_x              â”‚
  â”‚ 5. RNN: [obs_t, z_t, a_prev] â†’ h_next                   â”‚
  â”‚ 6. Loss: NLL + KL divergence                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **3. Multi-Head CausalGAT Communication**
```
Input: V_nodes = [h_next, zs] (N, hidden_dim + z_dim)
       prev_gat_output (N, head_dim) [optional]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head CausalGATLayer:                              â”‚
â”‚                                                         â”‚
â”‚ Shared Transform:                                       â”‚
â”‚   W1(V) â†’ Wh1 (N, hid_dim)                             â”‚
â”‚   W2(Wh1) â†’ Wh2 (N, out_dim)                           â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 1: Standard Attention                          â”‚ â”‚
â”‚ â”‚   src1 + dst1 + delta_bias â†’ alpha1                 â”‚ â”‚
â”‚ â”‚   H1 = alpha1 @ Wh1 â†’ H1_proj (N, head_dim)        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 2: Causal Attention                            â”‚ â”‚
â”‚ â”‚   For each pair (i,j):                              â”‚ â”‚
â”‚ â”‚     causal_encoder([V[i], V[j]]) â†’ causal_feat     â”‚ â”‚
â”‚ â”‚     causal_attn(causal_feat) â†’ causal_score        â”‚ â”‚
â”‚ â”‚   causal_scores â†’ alpha2 â†’ H2_proj (N, head_dim)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 3: Temporal Attention                          â”‚ â”‚
â”‚ â”‚   temporal_encoder([V, prev_hidden]) â†’ temp_feat   â”‚ â”‚
â”‚ â”‚   temporal_attn(temp_feat) â†’ temp_scores           â”‚ â”‚
â”‚ â”‚   temp_scores â†’ alpha3 â†’ H3_proj (N, head_dim)     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Head 4: Situation-aware Attention                   â”‚ â”‚
â”‚ â”‚   situation_encoder(V) â†’ situation_feat            â”‚ â”‚
â”‚ â”‚   situation_attn(situation_feat) â†’ situation_scoresâ”‚ â”‚
â”‚ â”‚   situation_scores â†’ alpha4 â†’ H4_proj (N, head_dim)â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚ Head Fusion:                                            â”‚
â”‚   [H1_proj, H2_proj, H3_proj, H4_proj] â†’ H_concat     â”‚
â”‚   head_fusion(H_concat) â†’ H_fused                      â”‚
â”‚   layer_norm(H_fused) â†’ H_output                       â”‚
â”‚                                                         â”‚
â”‚ Output: H_output (N, out_dim)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **4. Policy Generation**
```
For each agent i:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Policy Head i:                                          â”‚
  â”‚                                                         â”‚
  â”‚ Input: [obs[i], h_next[i], V_gat[i]]                   â”‚
  â”‚        (obs_dim + hidden_dim + gat_dim)                â”‚
  â”‚                                                         â”‚
  â”‚ Actor: input â†’ action_logits (act_dim)                  â”‚
  â”‚ Critic: input â†’ value (1)                               â”‚
  â”‚                                                         â”‚
  â”‚ Output: policy_logits, value                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ **Loss Functions**

### **1. VAE Loss**
```
L_VAE = nll_coef Ã— NLL + kl_coef Ã— KL + coop_coef Ã— Cooperative_KL

- NLL: Negative Log-Likelihood (reconstruction loss)
- KL: KL divergence between prior and posterior
- Cooperative_KL: Pairwise KL between agent latents
```

### **2. RL Loss**
```
L_RL = Policy_Loss + value_coef Ã— Value_Loss - ent_coef Ã— Entropy

- Policy_Loss: -(log_prob Ã— advantage).mean()
- Value_Loss: MSE(value_pred, returns)
- Entropy: -(probs Ã— log_probs).sum(-1).mean()
```

### **3. Communication Loss**
```
L_Comm = 0 (í˜„ì¬ëŠ” ë¹„í™œì„±í™”)

- Communicationì„ GATë¡œ ëŒ€ì²´í•˜ì—¬ ë‹¨ìˆœí™”
```

### **4. Adaptive Loss Balancing**
```
vae_coef, rl_coef, comm_coef = adaptive_loss_coefficients(loss_vae, loss_rl, loss_comm)
L_Total = vae_coef Ã— L_VAE + rl_coef Ã— L_RL + comm_coef Ã— L_Comm
```

## ğŸ“Š **ëª¨ë¸ í†µê³„**

### **íŒŒë¼ë¯¸í„° ë¶„í¬ (64:48:24 ì„¤ì •, 4 heads)**
```
VRNNCell:                   60,544 (57%)
Multi-Head CausalGATLayer:  18,432 (17%)
Policy Heads:               32,040 (30%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                     111,016 (100%)
```

### **ì°¨ì› ì •ë³´**
```
Input:     obs_dim = 16
Hidden:    hidden_dim = 64
Latent:    z_dim = 24
GAT:       gat_dim = 48
Head:      head_dim = 12 (gat_dim // 4)
Action:    act_dim = 3
Agents:    n_agents = 2
Heads:     n_heads = 4
```

## ğŸ”§ **ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**

### **ëª¨ë¸ êµ¬ì¡°**
- `hidden_dim : gat_dim : z_dim = 2.7 : 2 : 1`
- `n_heads = 4`: 4ê°œì˜ ë…ë¦½ì ì¸ attention head
- `head_dim = gat_dim // n_heads = 12`: ê° headì˜ ì°¨ì›
- Rolling window size: 10
- JSD update frequency: ë§¤ ìŠ¤í…

### **Multi-Head CausalGATLayer ì„¤ì •**
- **Head 1**: Standard attention (ê¸°ë³¸ GAT + delta bias)
- **Head 2**: Causal attention (pairwise relationships)
- **Head 3**: Temporal attention (temporal dependencies)
- **Head 4**: Situation-aware attention (context-dependent)
- **Head Fusion**: 4ê°œ head ì¶œë ¥ ê²°í•©
- **Layer Normalization**: í•™ìŠµ ì•ˆì •ì„±
- **Dropout**: 0.6

### **í•™ìŠµ ì„¤ì •**
- Learning rate: 3e-4 (RL), 1e-4 (VAE)
- Loss coefficients: nll_coef=1.0, kl_coef=0.1, coop_coef=0.01
- GAE parameters: Î³=0.99, Î»=0.95
- Gradient clipping: max_grad_norm=0.5
- EMA: ema_alpha=0.99

## ğŸš€ **ì‚¬ìš©ë²•**

### **ëª¨ë¸ ìƒì„±**
```python
model = VRNNGATA2C(
    obs_dim=16,
    act_dim=3,
    hidden_dim=64,
    z_dim=24,
    gat_dim=48,
    n_agents=2,
    use_gat=True,
    use_causal_gat=True,  # Multi-head CausalGATLayer ì‚¬ìš©
)
```

### **Forward Pass**
```python
h_next, nlls, kls, logits, ref_logits, values, mus, logvars, zs, V_gat, comm_recons = \
    model.forward_step(obs, a_prev, h_prev, rolling_mean_error)
```

### **Config ì„¤ì •**
```yaml
dectiger:
  use_causal_gat: true
  n_heads: 4
  max_grad_norm: 0.5
  ema_alpha: 0.99
```

## ğŸ¯ **ì¥ì **

1. **ë‹¤ì–‘í•œ ì¶”ë¡  ëŠ¥ë ¥**: 4ê°œì˜ ë…ë¦½ì ì¸ attention headë¡œ ì„œë¡œ ë‹¤ë¥¸ ì¶”ë¡  ìˆ˜í–‰
2. **ì¸ê³¼ê´€ê³„ ëª¨ë¸ë§**: Causal attentionìœ¼ë¡œ pairwise relationships í•™ìŠµ
3. **ì‹œê°„ì  ì˜ì¡´ì„±**: Temporal attentionìœ¼ë¡œ ì‹œí€€ìŠ¤ ì •ë³´ í™œìš©
4. **ìƒí™© ì¸ì‹**: Situation-aware attentionìœ¼ë¡œ context ê³ ë ¤
5. **í•™ìŠµ ì•ˆì •ì„±**: Layer normalization, dropout, head fusion
6. **í‘œí˜„ë ¥**: VRNNìœ¼ë¡œ ì‹œí€€ìŠ¤ ì •ë³´ ëª¨ë¸ë§
7. **í†µì‹ **: GATë¡œ íš¨ìœ¨ì ì¸ agent ê°„ ì •ë³´ êµí™˜
8. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ablation study ì§€ì›
9. **í™•ì¥ì„±**: ë‹¤ì–‘í•œ í™˜ê²½ì— ì ìš© ê°€ëŠ¥

## ğŸ”¬ **Ablation Study ì˜µì…˜**

1. **Headë³„ Ablation**: ê° attention head ê°œë³„ ë¹„í™œì„±í™”
2. **Standard GAT vs Multi-Head CausalGAT**: `use_causal_gat`
3. **GAT Ablation**: `use_gat=False`
4. **Communication Ablation**: Communication loss ì œê±°
5. **Temporal Reasoning Ablation**: prev_hidden=None
6. **Head Fusion Ablation**: ë‹¨ìˆœ concatenation vs fusion layer
7. **Layer Norm Ablation**: normalization ì œê±°

## ğŸ§  **ê° Headì˜ ì—­í• **

### **Head 1: Standard Attention**
- ê¸°ë³¸ì ì¸ GAT attention mechanism
- Rolling error biasë¡œ ì˜ˆì¸¡ ì˜¤ì°¨ ë°˜ì˜
- ì—ì´ì „íŠ¸ ê°„ ê¸°ë³¸ì ì¸ ì •ë³´ êµí™˜

### **Head 2: Causal Attention**
- Pairwise causal relationships ëª¨ë¸ë§
- ì—ì´ì „íŠ¸ ê°„ ì¸ê³¼ê´€ê³„ í•™ìŠµ
- "Aê°€ Bì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥" ê°™ì€ ì¸ê³¼ì  ê´€ê³„ íŒŒì•…

### **Head 3: Temporal Attention**
- ì‹œê°„ì  ì˜ì¡´ì„± ëª¨ë¸ë§
- ì´ì „ GAT ì¶œë ¥ì„ í™œìš©í•œ temporal reasoning
- ì‹œí€€ìŠ¤ ì •ë³´ì˜ ì‹œê°„ì  íŒ¨í„´ í•™ìŠµ

### **Head 4: Situation-aware Attention**
- í˜„ì¬ ìƒí™©ì— ë”°ë¥¸ context-dependent attention
- ê° ì—ì´ì „íŠ¸ì˜ ìƒí™©ì„ ê³ ë ¤í•œ attention
- í™˜ê²½ ë³€í™”ì— ë”°ë¥¸ ì ì‘ì  ì •ë³´ êµí™˜

---